{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6234d567",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/serverai/ltdoanh/pi0_vggt/Isaac-GR00T')  # noqa: E402"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d12b2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# libero_data_config.py\n",
    "\n",
    "from gr00t.experiment.data_config import BaseDataConfig, ModalityConfig\n",
    "from gr00t.data.transform import (\n",
    "    VideoToTensor,\n",
    "    VideoResize,\n",
    "    VideoColorJitter,\n",
    "    VideoToNumpy,\n",
    "    StateActionToTensor,\n",
    "    ComposedModalityTransform,   # ✅ thêm cái này\n",
    ")\n",
    "\n",
    "class LiberoDataConfig(BaseDataConfig):\n",
    "\n",
    "    video_keys = [\"video.image\", \"video.wrist_image\"]\n",
    "\n",
    "    state_keys = [\n",
    "        \"state.x\",\n",
    "        \"state.y\",\n",
    "        \"state.z\",\n",
    "        \"state.roll\",\n",
    "        \"state.pitch\",\n",
    "        \"state.yaw\",\n",
    "        \"state.gripper\",\n",
    "    ]\n",
    "\n",
    "    action_keys = [\n",
    "        \"action.x\",\n",
    "        \"action.y\",\n",
    "        \"action.z\",\n",
    "        \"action.roll\",\n",
    "        \"action.pitch\",\n",
    "        \"action.yaw\",\n",
    "        \"action.gripper\",\n",
    "    ]\n",
    "\n",
    "    def modality_config(self):\n",
    "        return {\n",
    "            \"video\": ModalityConfig(\n",
    "                modality_keys=self.video_keys,\n",
    "                delta_indices=[0],\n",
    "            ),\n",
    "            \"state\": ModalityConfig(\n",
    "                modality_keys=self.state_keys,\n",
    "                delta_indices=[0],\n",
    "            ),\n",
    "            \"action\": ModalityConfig(\n",
    "                modality_keys=self.action_keys,\n",
    "                delta_indices=[0],\n",
    "            ),\n",
    "        }\n",
    "\n",
    "    def transform(self) -> ComposedModalityTransform:\n",
    "        transforms = [\n",
    "            # --- VIDEO ---\n",
    "            VideoToTensor(apply_to=self.video_keys),\n",
    "            VideoResize(apply_to=self.video_keys, height=256, width=256),\n",
    "            VideoColorJitter(\n",
    "                apply_to=self.video_keys,\n",
    "                brightness=0.3,\n",
    "                contrast=0.4,\n",
    "                saturation=0.5,\n",
    "                hue=0.1,\n",
    "            ),\n",
    "            VideoToNumpy(apply_to=self.video_keys),\n",
    "\n",
    "            # --- STATE + ACTION → tensor ---\n",
    "            StateActionToTensor(\n",
    "                apply_to=self.state_keys + self.action_keys\n",
    "            ),\n",
    "        ]\n",
    "\n",
    "        # ❗ Quan trọng: dùng ComposedModalityTransform, không dùng ModalityTransform\n",
    "        return ComposedModalityTransform(transforms=transforms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5078618",
   "metadata": {},
   "outputs": [],
   "source": [
    "# libero_data_config.py\n",
    "\n",
    "from gr00t.experiment.data_config import BaseDataConfig, ModalityConfig, ModalityTransform\n",
    "from gr00t.data.transform import (\n",
    "    VideoToTensor,\n",
    "    VideoResize,\n",
    "    VideoColorJitter,\n",
    "    StateActionToTensor,\n",
    ")\n",
    "from torchvision import transforms as T\n",
    "\n",
    "class LiberoDataConfig(BaseDataConfig):\n",
    "    video_keys = [\n",
    "        \"video.image\",\n",
    "        \"video.wrist_image\",\n",
    "        \"video.image_depth\",\n",
    "        \"video.wrist_depth\",\n",
    "        \"video.image_mask\",\n",
    "        \"video.wrist_mask\",\n",
    "        \"video.object_of_interest_mask\",\n",
    "        \"video.object_of_interest_wrist_mask\",\n",
    "    ]\n",
    "\n",
    "    state_keys = [\n",
    "        \"state.x\",\"state.y\",\"state.z\",\n",
    "        \"state.roll\",\"state.pitch\",\"state.yaw\",\n",
    "        \"state.gripper\",\n",
    "    ]\n",
    "\n",
    "    action_keys = [\n",
    "        \"action.x\",\"action.y\",\"action.z\",\n",
    "        \"action.roll\",\"action.pitch\",\"action.yaw\",\n",
    "        \"action.gripper\",\n",
    "    ]\n",
    "\n",
    "    def modality_config(self):\n",
    "        return {\n",
    "            \"video\": ModalityConfig(\n",
    "                modality_keys=self.video_keys,\n",
    "                delta_indices=[0],   # vẫn lấy 1 frame hiện tại\n",
    "            ),\n",
    "            \"state\": ModalityConfig(\n",
    "                modality_keys=self.state_keys,\n",
    "                delta_indices=[0],\n",
    "            ),\n",
    "            \"action\": ModalityConfig(\n",
    "                modality_keys=self.action_keys,\n",
    "                delta_indices=[0],\n",
    "            ),\n",
    "        }\n",
    "\n",
    "    def transform(self) -> ComposedModalityTransform:\n",
    "        transforms = [\n",
    "            # --- tất cả video (RGB + depth + mask) ---\n",
    "            VideoToTensor(apply_to=self.video_keys),\n",
    "            VideoResize(apply_to=self.video_keys, height=256, width=256),\n",
    "            VideoColorJitter(\n",
    "                apply_to=[\"video.image\", \"video.wrist_image\"],  # chỉ augment RGB\n",
    "                brightness=0.3,\n",
    "                contrast=0.4,\n",
    "                saturation=0.5,\n",
    "                hue=0.1,\n",
    "            ),\n",
    "            VideoToNumpy(apply_to=self.video_keys),\n",
    "\n",
    "            # --- state/action ---\n",
    "            StateActionToTensor(\n",
    "                apply_to=self.state_keys + self.action_keys\n",
    "            ),\n",
    "        ]\n",
    "        return ComposedModalityTransform(transforms=transforms)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe836595",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized dataset merged_libero_mask_depth_noops_lerobot_40 with EmbodimentTag.NEW_EMBODIMENT\n"
     ]
    }
   ],
   "source": [
    "from gr00t.data.dataset import LeRobotSingleDataset\n",
    "from gr00t.data.schema import EmbodimentTag\n",
    "\n",
    "DATASET_PATH = \"./merged_libero_mask_depth_noops_lerobot_40\"\n",
    "\n",
    "data_config = LiberoDataConfig()\n",
    "modality_config = data_config.modality_config()\n",
    "modality_transform = data_config.transform()\n",
    "\n",
    "dataset = LeRobotSingleDataset(\n",
    "    dataset_path=DATASET_PATH,\n",
    "    modality_configs=modality_config,\n",
    "    video_backend=\"torchvision_av\",\n",
    "    video_backend_kwargs=None,\n",
    "    transforms=None,\n",
    "    embodiment_tag=EmbodimentTag.NEW_EMBODIMENT,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2cd71a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02db0eea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained dual brain from nvidia/GR00T-N1.5-3B\n",
      "Tune backbone vision tower: False\n",
      "Tune backbone LLM: False\n",
      "Tune action head projector: True\n",
      "Tune action head DiT: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 13 files: 100%|██████████| 13/13 [00:00<00:00, 103661.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tune backbone llm: False\n",
      "Tune backbone visual: True\n",
      "Total number of DiT parameters:  550386688\n",
      "Total number of SelfAttentionTransformer parameters:  201433088\n",
      "Tune action head projector: True\n",
      "Tune action head diffusion model: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:03<00:00,  1.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tune backbone llm: False\n",
      "Tune backbone visual: False\n",
      "Warning: No backbone trainable parameters found.\n",
      "Tune action head projector: True\n",
      "Tune action head diffusion model: True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GR00T_N1_5(\n",
       "  (backbone): EagleBackbone(\n",
       "    (eagle_model): Eagle2_5_VLForConditionalGeneration(\n",
       "      (vision_model): SiglipVisionModel(\n",
       "        (vision_model): SiglipVisionTransformer(\n",
       "          (embeddings): SiglipVisionEmbeddings(\n",
       "            (patch_embedding): Conv2d(3, 1152, kernel_size=(14, 14), stride=(14, 14), padding=valid)\n",
       "            (position_embedding): Embedding(256, 1152)\n",
       "          )\n",
       "          (encoder): SiglipEncoder(\n",
       "            (layers): ModuleList(\n",
       "              (0-26): 27 x SiglipEncoderLayer(\n",
       "                (layer_norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "                (self_attn): SiglipAttention(\n",
       "                  (k_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "                  (v_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "                  (q_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "                  (out_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "                )\n",
       "                (layer_norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "                (mlp): SiglipMLP(\n",
       "                  (activation_fn): PytorchGELUTanh()\n",
       "                  (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
       "                  (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (post_layernorm): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "          (head): SiglipMultiheadAttentionPoolingHead(\n",
       "            (attention): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1152, out_features=1152, bias=True)\n",
       "            )\n",
       "            (layernorm): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): SiglipMLP(\n",
       "              (activation_fn): PytorchGELUTanh()\n",
       "              (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
       "              (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (language_model): Qwen3ForCausalLM(\n",
       "        (model): Qwen3Model(\n",
       "          (embed_tokens): Embedding(151680, 2048)\n",
       "          (layers): ModuleList(\n",
       "            (0-11): 12 x Qwen3DecoderLayer(\n",
       "              (self_attn): Qwen3Attention(\n",
       "                (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                (k_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "                (v_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "                (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "                (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "              )\n",
       "              (mlp): Qwen3MLP(\n",
       "                (gate_proj): Linear(in_features=2048, out_features=6144, bias=False)\n",
       "                (up_proj): Linear(in_features=2048, out_features=6144, bias=False)\n",
       "                (down_proj): Linear(in_features=6144, out_features=2048, bias=False)\n",
       "                (act_fn): SiLU()\n",
       "              )\n",
       "              (input_layernorm): Qwen3RMSNorm((2048,), eps=1e-06)\n",
       "              (post_attention_layernorm): Qwen3RMSNorm((2048,), eps=1e-06)\n",
       "            )\n",
       "          )\n",
       "          (norm): Qwen3RMSNorm((2048,), eps=1e-06)\n",
       "          (rotary_emb): Qwen3RotaryEmbedding()\n",
       "        )\n",
       "        (lm_head): Linear(in_features=2048, out_features=151680, bias=False)\n",
       "      )\n",
       "      (mlp1): Sequential(\n",
       "        (0): Linear(in_features=1152, out_features=2048, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (eagle_linear): Identity()\n",
       "  )\n",
       "  (action_head): FlowmatchingActionHead(\n",
       "    (model): DiT(\n",
       "      (timestep_encoder): TimestepEncoder(\n",
       "        (time_proj): Timesteps()\n",
       "        (timestep_embedder): TimestepEmbedding(\n",
       "          (linear_1): Linear(in_features=256, out_features=1536, bias=True)\n",
       "          (act): SiLU()\n",
       "          (linear_2): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (transformer_blocks): ModuleList(\n",
       "        (0): BasicTransformerBlock(\n",
       "          (norm1): AdaLayerNorm(\n",
       "            (silu): SiLU()\n",
       "            (linear): Linear(in_features=1536, out_features=3072, bias=True)\n",
       "            (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=False)\n",
       "          )\n",
       "          (attn1): Attention(\n",
       "            (to_q): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (to_k): Linear(in_features=2048, out_features=1536, bias=True)\n",
       "            (to_v): Linear(in_features=2048, out_features=1536, bias=True)\n",
       "            (to_out): ModuleList(\n",
       "              (0): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "              (1): Dropout(p=0.2, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (norm3): LayerNorm((1536,), eps=1e-05, elementwise_affine=False)\n",
       "          (ff): FeedForward(\n",
       "            (net): ModuleList(\n",
       "              (0): GELU(\n",
       "                (proj): Linear(in_features=1536, out_features=6144, bias=True)\n",
       "              )\n",
       "              (1): Dropout(p=0.2, inplace=False)\n",
       "              (2): Linear(in_features=6144, out_features=1536, bias=True)\n",
       "              (3): Dropout(p=0.2, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (final_dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "        (1): BasicTransformerBlock(\n",
       "          (norm1): AdaLayerNorm(\n",
       "            (silu): SiLU()\n",
       "            (linear): Linear(in_features=1536, out_features=3072, bias=True)\n",
       "            (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=False)\n",
       "          )\n",
       "          (attn1): Attention(\n",
       "            (to_q): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (to_k): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (to_v): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (to_out): ModuleList(\n",
       "              (0): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "              (1): Dropout(p=0.2, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (norm3): LayerNorm((1536,), eps=1e-05, elementwise_affine=False)\n",
       "          (ff): FeedForward(\n",
       "            (net): ModuleList(\n",
       "              (0): GELU(\n",
       "                (proj): Linear(in_features=1536, out_features=6144, bias=True)\n",
       "              )\n",
       "              (1): Dropout(p=0.2, inplace=False)\n",
       "              (2): Linear(in_features=6144, out_features=1536, bias=True)\n",
       "              (3): Dropout(p=0.2, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (final_dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "        (2): BasicTransformerBlock(\n",
       "          (norm1): AdaLayerNorm(\n",
       "            (silu): SiLU()\n",
       "            (linear): Linear(in_features=1536, out_features=3072, bias=True)\n",
       "            (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=False)\n",
       "          )\n",
       "          (attn1): Attention(\n",
       "            (to_q): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (to_k): Linear(in_features=2048, out_features=1536, bias=True)\n",
       "            (to_v): Linear(in_features=2048, out_features=1536, bias=True)\n",
       "            (to_out): ModuleList(\n",
       "              (0): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "              (1): Dropout(p=0.2, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (norm3): LayerNorm((1536,), eps=1e-05, elementwise_affine=False)\n",
       "          (ff): FeedForward(\n",
       "            (net): ModuleList(\n",
       "              (0): GELU(\n",
       "                (proj): Linear(in_features=1536, out_features=6144, bias=True)\n",
       "              )\n",
       "              (1): Dropout(p=0.2, inplace=False)\n",
       "              (2): Linear(in_features=6144, out_features=1536, bias=True)\n",
       "              (3): Dropout(p=0.2, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (final_dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "        (3): BasicTransformerBlock(\n",
       "          (norm1): AdaLayerNorm(\n",
       "            (silu): SiLU()\n",
       "            (linear): Linear(in_features=1536, out_features=3072, bias=True)\n",
       "            (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=False)\n",
       "          )\n",
       "          (attn1): Attention(\n",
       "            (to_q): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (to_k): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (to_v): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (to_out): ModuleList(\n",
       "              (0): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "              (1): Dropout(p=0.2, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (norm3): LayerNorm((1536,), eps=1e-05, elementwise_affine=False)\n",
       "          (ff): FeedForward(\n",
       "            (net): ModuleList(\n",
       "              (0): GELU(\n",
       "                (proj): Linear(in_features=1536, out_features=6144, bias=True)\n",
       "              )\n",
       "              (1): Dropout(p=0.2, inplace=False)\n",
       "              (2): Linear(in_features=6144, out_features=1536, bias=True)\n",
       "              (3): Dropout(p=0.2, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (final_dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "        (4): BasicTransformerBlock(\n",
       "          (norm1): AdaLayerNorm(\n",
       "            (silu): SiLU()\n",
       "            (linear): Linear(in_features=1536, out_features=3072, bias=True)\n",
       "            (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=False)\n",
       "          )\n",
       "          (attn1): Attention(\n",
       "            (to_q): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (to_k): Linear(in_features=2048, out_features=1536, bias=True)\n",
       "            (to_v): Linear(in_features=2048, out_features=1536, bias=True)\n",
       "            (to_out): ModuleList(\n",
       "              (0): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "              (1): Dropout(p=0.2, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (norm3): LayerNorm((1536,), eps=1e-05, elementwise_affine=False)\n",
       "          (ff): FeedForward(\n",
       "            (net): ModuleList(\n",
       "              (0): GELU(\n",
       "                (proj): Linear(in_features=1536, out_features=6144, bias=True)\n",
       "              )\n",
       "              (1): Dropout(p=0.2, inplace=False)\n",
       "              (2): Linear(in_features=6144, out_features=1536, bias=True)\n",
       "              (3): Dropout(p=0.2, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (final_dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "        (5): BasicTransformerBlock(\n",
       "          (norm1): AdaLayerNorm(\n",
       "            (silu): SiLU()\n",
       "            (linear): Linear(in_features=1536, out_features=3072, bias=True)\n",
       "            (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=False)\n",
       "          )\n",
       "          (attn1): Attention(\n",
       "            (to_q): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (to_k): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (to_v): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (to_out): ModuleList(\n",
       "              (0): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "              (1): Dropout(p=0.2, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (norm3): LayerNorm((1536,), eps=1e-05, elementwise_affine=False)\n",
       "          (ff): FeedForward(\n",
       "            (net): ModuleList(\n",
       "              (0): GELU(\n",
       "                (proj): Linear(in_features=1536, out_features=6144, bias=True)\n",
       "              )\n",
       "              (1): Dropout(p=0.2, inplace=False)\n",
       "              (2): Linear(in_features=6144, out_features=1536, bias=True)\n",
       "              (3): Dropout(p=0.2, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (final_dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "        (6): BasicTransformerBlock(\n",
       "          (norm1): AdaLayerNorm(\n",
       "            (silu): SiLU()\n",
       "            (linear): Linear(in_features=1536, out_features=3072, bias=True)\n",
       "            (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=False)\n",
       "          )\n",
       "          (attn1): Attention(\n",
       "            (to_q): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (to_k): Linear(in_features=2048, out_features=1536, bias=True)\n",
       "            (to_v): Linear(in_features=2048, out_features=1536, bias=True)\n",
       "            (to_out): ModuleList(\n",
       "              (0): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "              (1): Dropout(p=0.2, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (norm3): LayerNorm((1536,), eps=1e-05, elementwise_affine=False)\n",
       "          (ff): FeedForward(\n",
       "            (net): ModuleList(\n",
       "              (0): GELU(\n",
       "                (proj): Linear(in_features=1536, out_features=6144, bias=True)\n",
       "              )\n",
       "              (1): Dropout(p=0.2, inplace=False)\n",
       "              (2): Linear(in_features=6144, out_features=1536, bias=True)\n",
       "              (3): Dropout(p=0.2, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (final_dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "        (7): BasicTransformerBlock(\n",
       "          (norm1): AdaLayerNorm(\n",
       "            (silu): SiLU()\n",
       "            (linear): Linear(in_features=1536, out_features=3072, bias=True)\n",
       "            (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=False)\n",
       "          )\n",
       "          (attn1): Attention(\n",
       "            (to_q): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (to_k): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (to_v): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (to_out): ModuleList(\n",
       "              (0): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "              (1): Dropout(p=0.2, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (norm3): LayerNorm((1536,), eps=1e-05, elementwise_affine=False)\n",
       "          (ff): FeedForward(\n",
       "            (net): ModuleList(\n",
       "              (0): GELU(\n",
       "                (proj): Linear(in_features=1536, out_features=6144, bias=True)\n",
       "              )\n",
       "              (1): Dropout(p=0.2, inplace=False)\n",
       "              (2): Linear(in_features=6144, out_features=1536, bias=True)\n",
       "              (3): Dropout(p=0.2, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (final_dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "        (8): BasicTransformerBlock(\n",
       "          (norm1): AdaLayerNorm(\n",
       "            (silu): SiLU()\n",
       "            (linear): Linear(in_features=1536, out_features=3072, bias=True)\n",
       "            (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=False)\n",
       "          )\n",
       "          (attn1): Attention(\n",
       "            (to_q): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (to_k): Linear(in_features=2048, out_features=1536, bias=True)\n",
       "            (to_v): Linear(in_features=2048, out_features=1536, bias=True)\n",
       "            (to_out): ModuleList(\n",
       "              (0): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "              (1): Dropout(p=0.2, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (norm3): LayerNorm((1536,), eps=1e-05, elementwise_affine=False)\n",
       "          (ff): FeedForward(\n",
       "            (net): ModuleList(\n",
       "              (0): GELU(\n",
       "                (proj): Linear(in_features=1536, out_features=6144, bias=True)\n",
       "              )\n",
       "              (1): Dropout(p=0.2, inplace=False)\n",
       "              (2): Linear(in_features=6144, out_features=1536, bias=True)\n",
       "              (3): Dropout(p=0.2, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (final_dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "        (9): BasicTransformerBlock(\n",
       "          (norm1): AdaLayerNorm(\n",
       "            (silu): SiLU()\n",
       "            (linear): Linear(in_features=1536, out_features=3072, bias=True)\n",
       "            (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=False)\n",
       "          )\n",
       "          (attn1): Attention(\n",
       "            (to_q): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (to_k): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (to_v): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (to_out): ModuleList(\n",
       "              (0): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "              (1): Dropout(p=0.2, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (norm3): LayerNorm((1536,), eps=1e-05, elementwise_affine=False)\n",
       "          (ff): FeedForward(\n",
       "            (net): ModuleList(\n",
       "              (0): GELU(\n",
       "                (proj): Linear(in_features=1536, out_features=6144, bias=True)\n",
       "              )\n",
       "              (1): Dropout(p=0.2, inplace=False)\n",
       "              (2): Linear(in_features=6144, out_features=1536, bias=True)\n",
       "              (3): Dropout(p=0.2, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (final_dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "        (10): BasicTransformerBlock(\n",
       "          (norm1): AdaLayerNorm(\n",
       "            (silu): SiLU()\n",
       "            (linear): Linear(in_features=1536, out_features=3072, bias=True)\n",
       "            (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=False)\n",
       "          )\n",
       "          (attn1): Attention(\n",
       "            (to_q): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (to_k): Linear(in_features=2048, out_features=1536, bias=True)\n",
       "            (to_v): Linear(in_features=2048, out_features=1536, bias=True)\n",
       "            (to_out): ModuleList(\n",
       "              (0): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "              (1): Dropout(p=0.2, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (norm3): LayerNorm((1536,), eps=1e-05, elementwise_affine=False)\n",
       "          (ff): FeedForward(\n",
       "            (net): ModuleList(\n",
       "              (0): GELU(\n",
       "                (proj): Linear(in_features=1536, out_features=6144, bias=True)\n",
       "              )\n",
       "              (1): Dropout(p=0.2, inplace=False)\n",
       "              (2): Linear(in_features=6144, out_features=1536, bias=True)\n",
       "              (3): Dropout(p=0.2, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (final_dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "        (11): BasicTransformerBlock(\n",
       "          (norm1): AdaLayerNorm(\n",
       "            (silu): SiLU()\n",
       "            (linear): Linear(in_features=1536, out_features=3072, bias=True)\n",
       "            (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=False)\n",
       "          )\n",
       "          (attn1): Attention(\n",
       "            (to_q): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (to_k): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (to_v): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (to_out): ModuleList(\n",
       "              (0): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "              (1): Dropout(p=0.2, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (norm3): LayerNorm((1536,), eps=1e-05, elementwise_affine=False)\n",
       "          (ff): FeedForward(\n",
       "            (net): ModuleList(\n",
       "              (0): GELU(\n",
       "                (proj): Linear(in_features=1536, out_features=6144, bias=True)\n",
       "              )\n",
       "              (1): Dropout(p=0.2, inplace=False)\n",
       "              (2): Linear(in_features=6144, out_features=1536, bias=True)\n",
       "              (3): Dropout(p=0.2, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (final_dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "        (12): BasicTransformerBlock(\n",
       "          (norm1): AdaLayerNorm(\n",
       "            (silu): SiLU()\n",
       "            (linear): Linear(in_features=1536, out_features=3072, bias=True)\n",
       "            (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=False)\n",
       "          )\n",
       "          (attn1): Attention(\n",
       "            (to_q): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (to_k): Linear(in_features=2048, out_features=1536, bias=True)\n",
       "            (to_v): Linear(in_features=2048, out_features=1536, bias=True)\n",
       "            (to_out): ModuleList(\n",
       "              (0): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "              (1): Dropout(p=0.2, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (norm3): LayerNorm((1536,), eps=1e-05, elementwise_affine=False)\n",
       "          (ff): FeedForward(\n",
       "            (net): ModuleList(\n",
       "              (0): GELU(\n",
       "                (proj): Linear(in_features=1536, out_features=6144, bias=True)\n",
       "              )\n",
       "              (1): Dropout(p=0.2, inplace=False)\n",
       "              (2): Linear(in_features=6144, out_features=1536, bias=True)\n",
       "              (3): Dropout(p=0.2, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (final_dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "        (13): BasicTransformerBlock(\n",
       "          (norm1): AdaLayerNorm(\n",
       "            (silu): SiLU()\n",
       "            (linear): Linear(in_features=1536, out_features=3072, bias=True)\n",
       "            (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=False)\n",
       "          )\n",
       "          (attn1): Attention(\n",
       "            (to_q): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (to_k): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (to_v): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (to_out): ModuleList(\n",
       "              (0): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "              (1): Dropout(p=0.2, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (norm3): LayerNorm((1536,), eps=1e-05, elementwise_affine=False)\n",
       "          (ff): FeedForward(\n",
       "            (net): ModuleList(\n",
       "              (0): GELU(\n",
       "                (proj): Linear(in_features=1536, out_features=6144, bias=True)\n",
       "              )\n",
       "              (1): Dropout(p=0.2, inplace=False)\n",
       "              (2): Linear(in_features=6144, out_features=1536, bias=True)\n",
       "              (3): Dropout(p=0.2, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (final_dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "        (14): BasicTransformerBlock(\n",
       "          (norm1): AdaLayerNorm(\n",
       "            (silu): SiLU()\n",
       "            (linear): Linear(in_features=1536, out_features=3072, bias=True)\n",
       "            (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=False)\n",
       "          )\n",
       "          (attn1): Attention(\n",
       "            (to_q): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (to_k): Linear(in_features=2048, out_features=1536, bias=True)\n",
       "            (to_v): Linear(in_features=2048, out_features=1536, bias=True)\n",
       "            (to_out): ModuleList(\n",
       "              (0): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "              (1): Dropout(p=0.2, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (norm3): LayerNorm((1536,), eps=1e-05, elementwise_affine=False)\n",
       "          (ff): FeedForward(\n",
       "            (net): ModuleList(\n",
       "              (0): GELU(\n",
       "                (proj): Linear(in_features=1536, out_features=6144, bias=True)\n",
       "              )\n",
       "              (1): Dropout(p=0.2, inplace=False)\n",
       "              (2): Linear(in_features=6144, out_features=1536, bias=True)\n",
       "              (3): Dropout(p=0.2, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (final_dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "        (15): BasicTransformerBlock(\n",
       "          (norm1): AdaLayerNorm(\n",
       "            (silu): SiLU()\n",
       "            (linear): Linear(in_features=1536, out_features=3072, bias=True)\n",
       "            (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=False)\n",
       "          )\n",
       "          (attn1): Attention(\n",
       "            (to_q): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (to_k): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (to_v): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (to_out): ModuleList(\n",
       "              (0): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "              (1): Dropout(p=0.2, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (norm3): LayerNorm((1536,), eps=1e-05, elementwise_affine=False)\n",
       "          (ff): FeedForward(\n",
       "            (net): ModuleList(\n",
       "              (0): GELU(\n",
       "                (proj): Linear(in_features=1536, out_features=6144, bias=True)\n",
       "              )\n",
       "              (1): Dropout(p=0.2, inplace=False)\n",
       "              (2): Linear(in_features=6144, out_features=1536, bias=True)\n",
       "              (3): Dropout(p=0.2, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (final_dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm_out): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
       "      (proj_out_1): Linear(in_features=1536, out_features=3072, bias=True)\n",
       "      (proj_out_2): Linear(in_features=1536, out_features=1024, bias=True)\n",
       "    )\n",
       "    (state_encoder): CategorySpecificMLP(\n",
       "      (layer1): CategorySpecificLinear()\n",
       "      (layer2): CategorySpecificLinear()\n",
       "    )\n",
       "    (action_encoder): MultiEmbodimentActionEncoder(\n",
       "      (W1): CategorySpecificLinear()\n",
       "      (W2): CategorySpecificLinear()\n",
       "      (W3): CategorySpecificLinear()\n",
       "      (pos_encoding): SinusoidalPositionalEncoding()\n",
       "    )\n",
       "    (action_decoder): CategorySpecificMLP(\n",
       "      (layer1): CategorySpecificLinear()\n",
       "      (layer2): CategorySpecificLinear()\n",
       "    )\n",
       "    (future_tokens): Embedding(32, 1536)\n",
       "    (vlln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "    (vl_self_attention): SelfAttentionTransformer(\n",
       "      (transformer_blocks): ModuleList(\n",
       "        (0-3): 4 x BasicTransformerBlock(\n",
       "          (norm1): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn1): Attention(\n",
       "            (to_q): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "            (to_k): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "            (to_v): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "            (to_out): ModuleList(\n",
       "              (0): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "              (1): Dropout(p=0.2, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (norm3): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (ff): FeedForward(\n",
       "            (net): ModuleList(\n",
       "              (0): GELU(\n",
       "                (proj): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "              )\n",
       "              (1): Dropout(p=0.2, inplace=False)\n",
       "              (2): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "              (3): Dropout(p=0.2, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (final_dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (position_embedding): Embedding(1024, 1536)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gr00t.model.gr00t_n1 import GR00T_N1_5\n",
    "\n",
    "BASE_MODEL_PATH = \"nvidia/GR00T-N1.5-3B\"\n",
    "TUNE_LLM = False            # Whether to tune the LLM\n",
    "TUNE_VISUAL = False          # Whether to tune the visual encoder\n",
    "TUNE_PROJECTOR = True       # Whether to tune the projector\n",
    "TUNE_DIFFUSION_MODEL = True # Whether to tune the diffusion model\n",
    "\n",
    "model = GR00T_N1_5.from_pretrained(\n",
    "    pretrained_model_name_or_path=BASE_MODEL_PATH,\n",
    "    tune_llm=TUNE_LLM,  # backbone's LLM\n",
    "    tune_visual=TUNE_VISUAL,  # backbone's vision tower\n",
    "    tune_projector=TUNE_PROJECTOR,  # action head's projector\n",
    "    tune_diffusion_model=TUNE_DIFFUSION_MODEL,  # action head's DiT\n",
    ")\n",
    "\n",
    "# Set the model's compute_dtype to bfloat16\n",
    "model.compute_dtype = \"bfloat16\"\n",
    "model.config.compute_dtype = \"bfloat16\"\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "21a1b305",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "output_dir = \"output/model/path\"    # CHANGE THIS ACCORDING TO YOUR LOCAL PATH\n",
    "per_device_train_batch_size = 8     # CHANGE THIS ACCORDING TO YOUR GPU MEMORY\n",
    "max_steps = 20                      # CHANGE THIS ACCORDING TO YOUR NEEDS\n",
    "report_to = \"wandb\"\n",
    "dataloader_num_workers = 8\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    run_name=None,\n",
    "    remove_unused_columns=False,\n",
    "    deepspeed=\"\",\n",
    "    gradient_checkpointing=False,\n",
    "    bf16=True,\n",
    "    tf32=True,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    gradient_accumulation_steps=1,\n",
    "    dataloader_num_workers=dataloader_num_workers,\n",
    "    dataloader_pin_memory=False,\n",
    "    dataloader_persistent_workers=True,\n",
    "    optim=\"adamw_torch\",\n",
    "    adam_beta1=0.95,\n",
    "    adam_beta2=0.999,\n",
    "    adam_epsilon=1e-8,\n",
    "    learning_rate=1e-4,\n",
    "    weight_decay=1e-5,\n",
    "    warmup_ratio=0.05,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    logging_steps=10.0,\n",
    "    num_train_epochs=300,\n",
    "    max_steps=max_steps,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=500,\n",
    "    save_total_limit=8,\n",
    "    report_to=report_to,\n",
    "    seed=42,\n",
    "    do_eval=False,\n",
    "    ddp_find_unused_parameters=False,\n",
    "    ddp_bucket_cap_mb=100,\n",
    "    torch_compile_mode=None,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0d609541",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run name: output/model/path\n",
      "train dataloader length: 16732\n",
      "train dataset length: 133851\n",
      "GPU memory before training: 7.076685905456543 GB\n",
      "TensorBoard logs will be saved to: output/model/path/runs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlethiendoanh-work\u001b[0m (\u001b[33mlethiendoanh-work-international-university-vnu-hcmc\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/serverai/ltdoanh/pi0_vggt/wandb/run-20251124_163209-589a0gy9</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/lethiendoanh-work-international-university-vnu-hcmc/huggingface/runs/589a0gy9' target=\"_blank\">output/model/path</a></strong> to <a href='https://wandb.ai/lethiendoanh-work-international-university-vnu-hcmc/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/lethiendoanh-work-international-university-vnu-hcmc/huggingface' target=\"_blank\">https://wandb.ai/lethiendoanh-work-international-university-vnu-hcmc/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/lethiendoanh-work-international-university-vnu-hcmc/huggingface/runs/589a0gy9' target=\"_blank\">https://wandb.ai/lethiendoanh-work-international-university-vnu-hcmc/huggingface/runs/589a0gy9</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyError",
     "evalue": "'eagle_pixel_values'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 9\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgr00t\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperiment\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrunner\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TrainRunner\n\u001b[1;32m      3\u001b[0m experiment \u001b[38;5;241m=\u001b[39m TrainRunner(\n\u001b[1;32m      4\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mdataset,\n\u001b[1;32m      5\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m      6\u001b[0m     training_args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[1;32m      7\u001b[0m )\n\u001b[0;32m----> 9\u001b[0m \u001b[43mexperiment\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ltdoanh/pi0_vggt/Isaac-GR00T/gr00t/experiment/runner.py:173\u001b[0m, in \u001b[0;36mTrainRunner.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtrain\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    172\u001b[0m     \u001b[38;5;66;03m# Start training\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39msave_state()\n\u001b[1;32m    176\u001b[0m     safe_save_model_for_hf_trainer(\n\u001b[1;32m    177\u001b[0m         trainer\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer,\n\u001b[1;32m    178\u001b[0m         output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_args\u001b[38;5;241m.\u001b[39moutput_dir,\n\u001b[1;32m    179\u001b[0m     )\n",
      "File \u001b[0;32m~/ltdoanh/pi0_vggt/Isaac-GR00T/gr00t/experiment/trainer.py:159\u001b[0m, in \u001b[0;36mDualBrainTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m resume_from_checkpoint \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;66;03m# In case of repeating the find_executable_batch_size, set `self._train_batch_size` properly\u001b[39;00m\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m=\u001b[39m TrainerState\u001b[38;5;241m.\u001b[39mload_from_json(\n\u001b[1;32m    157\u001b[0m         os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(resume_from_checkpoint, TRAINER_STATE_NAME)\n\u001b[1;32m    158\u001b[0m     )\n\u001b[0;32m--> 159\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/srv/conda/envs/serverai/gr00t310/lib/python3.10/site-packages/transformers/trainer.py:2245\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2243\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2244\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2245\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2246\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2247\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2248\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2249\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2250\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/srv/conda/envs/serverai/gr00t310/lib/python3.10/site-packages/transformers/trainer.py:2560\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2553\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2554\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[1;32m   2555\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   2556\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m!=\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED\n\u001b[1;32m   2557\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[1;32m   2558\u001b[0m )\n\u001b[1;32m   2559\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[0;32m-> 2560\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2562\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2563\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2564\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2565\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2566\u001b[0m ):\n\u001b[1;32m   2567\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2568\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m/srv/conda/envs/serverai/gr00t310/lib/python3.10/site-packages/transformers/trainer.py:3736\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3733\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   3735\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 3736\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3738\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[1;32m   3739\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   3740\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   3741\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   3742\u001b[0m ):\n",
      "File \u001b[0;32m~/ltdoanh/pi0_vggt/Isaac-GR00T/gr00t/experiment/trainer.py:81\u001b[0m, in \u001b[0;36mDualBrainTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_loss\u001b[39m(\u001b[38;5;28mself\u001b[39m, model, inputs, return_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, num_items_in_batch\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m---> 81\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m     loss \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (loss, outputs) \u001b[38;5;28;01mif\u001b[39;00m return_outputs \u001b[38;5;28;01melse\u001b[39;00m loss\n",
      "File \u001b[0;32m/srv/conda/envs/serverai/gr00t310/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/srv/conda/envs/serverai/gr00t310/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/srv/conda/envs/serverai/gr00t310/lib/python3.10/site-packages/accelerate/utils/operations.py:819\u001b[0m, in \u001b[0;36mconvert_outputs_to_fp32.<locals>.forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 819\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/srv/conda/envs/serverai/gr00t310/lib/python3.10/site-packages/accelerate/utils/operations.py:807\u001b[0m, in \u001b[0;36mConvertOutputsToFp32.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    806\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 807\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m convert_to_fp32(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/srv/conda/envs/serverai/gr00t310/lib/python3.10/site-packages/torch/amp/autocast_mode.py:43\u001b[0m, in \u001b[0;36mautocast_decorator.<locals>.decorate_autocast\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_autocast\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m autocast_instance:\n\u001b[0;32m---> 43\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ltdoanh/pi0_vggt/Isaac-GR00T/gr00t/model/gr00t_n1.py:165\u001b[0m, in \u001b[0;36mGR00T_N1_5.forward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    163\u001b[0m     inputs: \u001b[38;5;28mdict\u001b[39m,\n\u001b[1;32m    164\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BatchFeature:\n\u001b[0;32m--> 165\u001b[0m     backbone_inputs, action_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprepare_input\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m     backbone_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackbone(backbone_inputs)\n\u001b[1;32m    167\u001b[0m     action_head_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_head(backbone_outputs, action_inputs)\n",
      "File \u001b[0;32m~/ltdoanh/pi0_vggt/Isaac-GR00T/gr00t/model/gr00t_n1.py:204\u001b[0m, in \u001b[0;36mGR00T_N1_5.prepare_input\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# ============================\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# LIBERO FIX PATCH\u001b[39;00m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;66;03m# ============================\u001b[39;00m\n\u001b[1;32m    201\u001b[0m \n\u001b[1;32m    202\u001b[0m \u001b[38;5;66;03m# 1) Add image_sizes for Eagle Vision Transformer\u001b[39;00m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meagle_image_sizes\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m backbone_inputs:\n\u001b[0;32m--> 204\u001b[0m     B \u001b[38;5;241m=\u001b[39m \u001b[43mbackbone_inputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meagle_pixel_values\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    205\u001b[0m     backbone_inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meagle_image_sizes\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\n\u001b[1;32m    206\u001b[0m         [[\u001b[38;5;241m256\u001b[39m, \u001b[38;5;241m256\u001b[39m]] \u001b[38;5;241m*\u001b[39m B,\n\u001b[1;32m    207\u001b[0m         dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong,\n\u001b[1;32m    208\u001b[0m         device\u001b[38;5;241m=\u001b[39mbackbone_inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meagle_pixel_values\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mdevice\n\u001b[1;32m    209\u001b[0m     )\n\u001b[1;32m    211\u001b[0m \u001b[38;5;66;03m# 2) Add pixel mask\u001b[39;00m\n",
      "File \u001b[0;32m/srv/conda/envs/serverai/gr00t310/lib/python3.10/site-packages/transformers/feature_extraction_utils.py:86\u001b[0m, in \u001b[0;36mBatchFeature.__getitem__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;124;03mIf the key is a string, returns the value of the dict associated to `key` ('input_values', 'attention_mask',\u001b[39;00m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;124;03metc.).\u001b[39;00m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(item, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m---> 86\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIndexing with integers is not available when using Python based feature extractors\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'eagle_pixel_values'"
     ]
    }
   ],
   "source": [
    "from gr00t.experiment.runner import TrainRunner\n",
    "\n",
    "experiment = TrainRunner(\n",
    "    train_dataset=dataset,\n",
    "    model=model,\n",
    "    training_args=training_args,\n",
    ")\n",
    "\n",
    "experiment.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4015417",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gr00t.experiment.data_config import DATA_CONFIG_MAP\n",
    "from gr00t.model.policy import Gr00tPolicy\n",
    "# 3. Load pretrained GR00T\n",
    "policy = Gr00tPolicy(\n",
    "    model_path=\"nvidia/GR00T-N1.5-3B\",\n",
    "    embodiment_tag=EmbodimentTag.NEW_EMBODIMENT,\n",
    "    modality_config=modality_config,\n",
    "    modality_transform=modality_transform,\n",
    "    device=\"cuda\",\n",
    ")\n",
    "\n",
    "# 4. Eval thử 1 traj\n",
    "from gr00t.utils.eval import calc_mse_for_single_trajectory\n",
    "\n",
    "mse = calc_mse_for_single_trajectory(\n",
    "    policy,\n",
    "    dataset,\n",
    "    traj_id=0,\n",
    "    modality_keys=[\"action\"],   # hoặc [\"right_arm\", \"right_hand\"] nếu LIBERO map kiểu đó\n",
    "    steps=150,\n",
    "    action_horizon=16,\n",
    ")\n",
    "print(\"MSE:\", mse)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gr00t310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
